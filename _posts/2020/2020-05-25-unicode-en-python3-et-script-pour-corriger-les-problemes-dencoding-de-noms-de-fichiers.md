---
post_id: 4368
title: 'Unicode en python3 et script pour corriger les problÃ¨mes d&#8217;encoding de noms de fichiers'
date: '2020-05-25T18:03:04+02:00'
last_modified_at: '2020-05-25T18:09:37+02:00'
author: 'RÃ©mi Peyronnet'
layout: post
guid: '/?p=4368'
slug: unicode-en-python3-et-script-pour-corriger-les-problemes-dencoding-de-noms-de-fichiers
permalink: /2020/05/unicode-en-python3-et-script-pour-corriger-les-problemes-dencoding-de-noms-de-fichiers/
image: /files/2020/05/800px-New_Unicode_logo.svg.png
categories:
    - Informatique
tags:
    - Python
    - UTF8
    - chardet
    - convmv
    - Filesystem
    - Perl
    - Script
lang: fr
term_language:
    - FranÃ§ais
term_translations:
    - pll_612cf7f113ebe
---

La gestion de lâ€™unicode a toujours Ã©tÃ© la galÃ¨re en programmation, et notamment en python. Il y a une vingtaine dâ€™annÃ©e avec les Windows-1252 ou latin1, au pire on se retrouvait un affichage bizarre mais cela marchait toujours. Avec la gÃ©nÃ©ralisation dâ€™UTF8, maintenant standard partout que ce soit pour lâ€™affichage sur les consoles ou les systÃ¨mes de fichiers la donne a changÃ©, et les scripts python se retrouvent souvent Ã  planter Ã  cause dâ€™un problÃ¨me dâ€™encoding, et les messages comme celui ci-dessous sont devenus lâ€™enfer du dÃ©veloppeur python, surtout avec python2 :

```
UnicodeEncodeError: 'ascii' codec can't encode character u'\xb7' in position 590: ordinal not in range(128)
```

Câ€™Ã©tait devenu tellement complexe avec les conversions explicites ou implicites quâ€™on se retrouvait alors Ã  tÃ¢tonner en ajoutant des `.encode` et des `.decode` un peu dans tous les sens jusquâ€™Ã  ce que Ã§a tombe en marcheâ€¦ Pas terribleâ€¦

# Trucs rapides pour maÃ®triser les encodings en python3

Python3 rationalise Ã§a et apporte une solution bien plus efficace en sÃ©parant proprement les chaines de caractÃ¨res unicode (str) des suites dâ€™octets (bytes), et des fonctions dâ€™encodage / dÃ©codage pour passer de lâ€™une Ã  lâ€™autre : str.encode -&gt; bytes, et bytes.decode -&gt; str. Pour faire la diffÃ©rence sur les chaÃ®nes littÃ©rales, il faut utiliser bâ€ devant les bytes et sinon python3 interprÃ©tera en str, en dÃ©codant depuis lâ€™encoding de votre fichier source, quâ€™il faut donc penser Ã  indiquer avec â€œ# coding: â€ en dÃ©but de script.

En utilisation normale python3 va donc parfaitement se comporter, faisant implicitement pour votre compte les traductions unicode -&gt; encoding lorsque cela est nÃ©cessaire, par exemple pour â€˜printâ€™, pour lâ€™ouverture dâ€™un fichier en mode texte, etc.

Malheureusement, si le contenu que vous traitez comporte des erreurs, python3 est toujours aussi intolÃ©rant et vous retrouverez les exeptions UnicodeEncodeError &amp; coâ€¦ Comme la distinction str / bytes est maintenant bien marquÃ©e il nâ€™est plus aussi simple quâ€™avant que de lâ€™embrouillerâ€¦ Heureusement il y a cependant quelques astuces pour ce faire. Votre bible est la [section python specific encodings de la page 7.2 codecs de la documentation python3](https://docs.python.org/3.3/library/codecs.html#python-specific-encodings). Vous y trouverez des codecs particuliers pour faire des choses pas trÃ¨s catholiques, et un peu plus haut dans la page, la liste des errors handlers possibles.

En gros Ã  chaque endroit oÃ¹ vous pouvez specifier un paramÃ¨tre `encoding` il existe Ã©galement un autre paramÃ¨tre `errors` qui va spÃ©cifier le comportement du codec en cas dâ€™erreur dâ€™encoding. Si vous ne spÃ©cifiez rien, cela correspond Ã  â€˜`strict` â€˜ et la fameuse exception sera levÃ©e. Dans la liste possible, â€˜`ignore` â€˜ est sans doute le plus simple : le caractÃ¨re fautif est simplement ignorÃ©. Vous pouvez redÃ©finir les options de stdout pour spÃ©cifier le comportement de print et ne plus avoir de plantage liÃ©s Ã  des print de debuggingâ€¦

```
# To avoid display problems of wrong encodings on utf8 terminal 
import sys 
sys.stdout.reconfigure(errors="ignore")
```

Si vous nâ€™avez pas besoin dâ€™un comportement exact câ€™est sans doute le plus simple, sinon il faut regarder de plus prÃ¨s les autres, qui vont tous substituer le caractÃ¨re fautif en une autre forme acceptÃ©e (le cas des \*replace), ou temporairement tolÃ©rÃ©e (le cas de surrogateescape). Le problÃ¨me se pose alors si vous avez besoin de retrouver la chaine de caractÃ¨re originale avec son problÃ¨me dâ€™encoding (câ€™est le cas des fichiers quâ€™on veut renommer). Pour sa gestion interne des systÃ¨mes de fichiers python3 semble avoir adoptÃ© la gestion du surrogateescape ; ainsi un os.listdir ne retournera pas dâ€™exception si un nom de fichier ne suit pas lâ€™encoding du filesystem mais la chaine de caractÃ¨re unicode comportera des caractÃ¨res â€˜surrogateâ€™. Lâ€™avantage sur surrogateescape est quâ€™il est simple de retrouver la forme dâ€™origine, il suffit dâ€™encoder en ajoutant le paramÃ¨tre `errors=â€surrogateescapeâ€` ; lâ€™inconvÃ©nient est que par dÃ©faut tout codec renverra une erreur sur les caractÃ¨res surrogate et quâ€™il faut donc spÃ©cifier systÃ©matiquement un comportement. Il existe â€œ`surrogatepass` â€ pour les ignorer. Donc par exemple pour afficher sur la console des chaines potentiellement avec surrogate :

```
sys.stdout.reconfigure(errors="surrogatepass")
```

Le plus confortable pour ne pas craindre Ã  chaque instant une exception est donc dâ€™utiliser un des modes â€˜\*replaceâ€™ qui va remplacer le caractÃ¨re fautif par une sÃ©quence dâ€™Ã©chappement acceptÃ©e. Il nâ€™est cependant pas toujours facile de revenir Ã  la version initiale. AprÃ¨s avoir galÃ©rÃ© un certain temps avec â€˜backslashreplaceâ€™, jâ€™ai trouvÃ© dans [ce fil stackoverflow](https://stackoverflow.com/questions/14820429/how-do-i-decodestring-escape-in-python3) une mÃ©thode simple pour le faire, qui se base en partie sur le codec unicode-escape, et en partie sur la capacitÃ© du codec latin1 de convertir 100% des bytes en string et inversement pour pallier le fonctionnement de unicode-escape uniquement sur des bytes. Pour inverser un â€˜backslashreplaceâ€™, la fonction est donc :

```
# Adaptation to revert backslashreplace content 
def backslashreplace_revert(s): 
  return (s.encode('latin1') # To bytes, required by 'unicode-escape' 
          .decode('unicode-escape') # Perform the actual octal-escaping decode 
          .encode('latin1') ) # 1:1 mapping back to bytes
```

# Assainir les noms de fichiers de son filesystem

Cette longue introduction pour prÃ©senter une application directe de ces Ã©lÃ©ments sur le cas qui mâ€™intÃ©resse, Ã  savoir pouvoir faire en sorte que mon filesystem ne comporte que des noms de fichiers valides en UTF8. En effet, au fil de plus de 20 ans, entre le passage initial Ã  utf8, les copies depuis des Windows plus ou moins au fait dâ€™utf8, la dÃ©compression de vieux fichiers zip mal encodÃ©s, jâ€™avais plus de 1000 noms de fichiers dont lâ€™encoding nâ€™Ã©tait pas correct. Ã‡a ne me gÃªnait pas outre mesure jusque lÃ , mais en voulant mettre en place un backup via [rclone](https://rclone.org/), ce dernier rÃ¢le copieusement lorsque le nom de fichier nâ€™est pas conformeâ€¦ Pas facile Ã  identifier et corriger Ã  la main, donc jâ€™ai fait un script.

Si identifier les fichiers fautifs est assez facile, câ€™est plus compliquÃ© de deviner le bon encoding dâ€™origine. Il existe un trÃ¨s bon outil [chardet](https://pypi.org/project/chardet/) qui permet dâ€™identifier lâ€™encoding dâ€™un texte inconnu. Malheureusement il ne supporte que quelques encodings et pas tous ceux que jâ€™avais sur mon disque, câ€™est Ã  dire Ã  peu prÃ¨s tous ceux utilisÃ©s pour du franÃ§ais : latin1/15 le classique europÃ©en, cp1252 la dÃ©clinaison par Windows, et cp850, lâ€™ancienne version sous DOS/Windows. Ce dernier nâ€™est pas supportÃ© par chardet (et lâ€™ajout dâ€™un nouvel encoding ne semble pas hyper simple dans cet outil). Il me fallait donc trouver un autre moyen. Jâ€™ai fait sale, mais Ã§a marche pas si mal ğŸ™‚ : jâ€™ai basiquement fait une liste des caractÃ¨res qui peuvent lÃ©gitimement se retrouver dans mes noms de fichiers. Si un encoding me sort un nom de fichier qui nâ€™utilise que ces caractÃ¨res il est alors sans doute correct, sinon il est sans doute faux. Je fais le test sur la liste de ceux que je suis susceptible de rencontrer, et si plusieurs matchent, je privilÃ©gie celui trouvÃ© par chardet qui est un peu plus intelligent que ce que jâ€™ai faitâ€¦ Si aucun ne correspond, je sÃ©lectionne celui qui comporte le moins de caractÃ¨res en Ã©cart et sort une alerte pour lâ€™utilisateur.

Pour les besoins de la mise au point de ce script, plutÃ´t que de scanner Ã  chaque fois mon disque jâ€™ai prÃ©fÃ©rÃ© travailler sur un fichier de lâ€™ensemble de la liste des fichiers, basiquement produit par `find /` . Si câ€™est super simple pour dÃ©tecter les fichiers fautifs, je me suis ensuite rendu compte quâ€™une structure Ã  plat serait compliquÃ©e Ã  traiter pour lâ€™Ã©tape de renommage. Jâ€™ai donc optÃ© pour une conversion du fichier Ã  plat en une structure arborescente, qui serait plus facile Ã  construire directement depuis le disque que depuis la liste des fichiers Ã  plat.

Enfin jâ€™aurais souhaitÃ© initialement pouvoir gÃ©nÃ©rer un script shell avec la liste des commandes mv plutÃ´t que de renommer les fichiers en python (ce que je fais dâ€™habitude et qui est plus pratique pour tester et plus sÃ©curisant). Malheureusement la fonction `shlex.quote` pour transformer une chaÃ®ne de caractÃ¨re en argument pour le shell ne semble pas se comporter Ã  lâ€™identique du shell sur les noms de fichiers avec un mauvais encoding, il Ã©tait donc plus simple de renommer en python directement plutÃ´t que de devoir implÃ©menter lâ€™Ã©chappement correct.

Voici donc le code rÃ©sultant :

```python
# coding: utf8
# python3 only because of unicode support

import codecs
import json
import pprint
import os
import argparse

# for charset detection : pip install chardet
import chardet

normalchars="/ " + \
	"ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz0123456789" + \
	"Ãƒâ€°ÃƒÅ Ãƒâ€¹Ãƒâ‚¬Ãƒ&nbsp;ÃƒÂ¢ÃƒÂ©ÃƒÂ¨ÃƒÂ«ÃƒÂªÃƒÂ¯ÃƒÂ®ÃƒÂ´ÃƒÂ¹ÃƒÂ»ÃƒÂ§ÃƒÂ±ÃƒÂ¢ÃƒÂ´ÃƒÂ»Ãƒâ€º" + \
	"!'Ã‚Â°~#$%_Ã‚Â§&()+,-.;=@[]{}Ã‚Â©Ã‚Â®"
unwantedchars="\"^`|*?<>:\\Ã‚Â²"

# defaults for debug
filelist='files.lst'
filerename='files.ren'

# To avoid display problems of wrong encodings on utf8 terminal
import sys
sys.stdout.reconfigure(errors="surrogatepass")

# From : https://stackoverflow.com/questions/14820429/how-do-i-decodestring-escape-in-python3
def string_escape(s, encoding='utf-8'):
    return (s.encode('latin1')         # To bytes, required by 'unicode-escape'
             .decode('unicode-escape') # Perform the actual octal-escaping decode
             .encode('latin1')         # 1:1 mapping back to bytes
             .decode(encoding,errors="surrogateescape"))        # Decode original encoding

# Adaptation to revert backslashreplace content
def backslashreplace_revert(s):
    return (s.encode('latin1')         # To bytes, required by 'unicode-escape'
             .decode('unicode-escape') # Perform the actual octal-escaping decode
             .encode('latin1') )        # 1:1 mapping back to bytes

# Utility function to len or 0 for None
def lenornone(var):
	if var:
		return len(var)
	else:
		return 0

# Utility function to check if filename is in the "authorized char list" 
#   (used to guess if it is right encoding or messed up...)
def check_filename(filename, setnormalchars):
	lineset = set(filename).difference(setnormalchars)
	if len(lineset) > 0:
		return lineset
	else:
		return None

# Get chars present in filenames that are not in the "authorized char list" 
def get_suspicious_chars(filelist, normalchars) :
	# Adapted from :  https://stackoverflow.com/questions/21522234/how-to-get-all-unique-characters-in-a-textfile-unix-python
	fh = open(filelist,'r', encoding="utf8", errors="backslashreplace").read()
	unique_chars = set(fh)
	suspicious = { (v if v not in normalchars else '')  for v in unique_chars }
	return suspicious

# List files that do not match the "authorized char list" 
def list_suspicious_files(filelist, setnormalchars):
	fh = open(filelist,'r', encoding="utf8", errors="ignore")
	for line in fh.read().splitlines():
		lineset = check_filename(line_utf8, setnormalchars)
		if lineset:
			print("# ", lineset)
			print(line)

# List files that present errors in UTF8
def list_utf8errors_files(filelist, setnormalchars):
	fh = open(filelist,'rb')
	for line in fh.read().splitlines():
		try: 
			line_utf8 = line.decode("utf8")
		except:
			# I should maybe have used replace, or surrogateescape, that would
			#   have been more easy to revert
			line_utf8 = line.decode("utf8", errors="backslashreplace")
			print("> ", line_utf8)
			detenc = chardet.detect(line)
			sel_encoding = None;
			for encoding in {"latin1", "cp1252", "cp850", detenc['encoding']}:
				line_enc = line.decode(encoding, errors="ignore")
				line_enc_set = check_filename(line_enc, setnormalchars)
				if (sel_encoding is None) or (sel_encoding and (lenornone(line_enc_set) < lenornone(sel_enc_set))):
					sel_encoding = encoding
					sel_enc_set = line_enc_set
			line_enc = line.decode(sel_encoding, errors="ignore")
			if lenornone(sel_enc_set) > 0:
				print("# !!! No suitable encoding found !!! Encoding: ", encoding," Set: ", sel_enc_set)
			else:
				print("# Encoding: ",  encoding)
			# shlex.quote do not seems to be working at all for encoding errors, 
			#    so we cannot create mv script as initiall intended
			#    (would have been easier to work with)
			# print("mv ", shlex.quote(line_utf8), shlex.quote(line_enc))
			print("Selected: ", line_enc)

# Clean the tree for empty renaming			
def clean_tree(tree):
	for key in list(tree.keys()):
		item = tree[key]
		if (key != '.') and (key != '..'):
			clean_tree(item)
		if (len(item) == 1) and (len(item['.']) == 0):
			del tree[key]

# Convert filename list to tree, because :
#  - for a huge bunch of files, it is more easy to work with a file 
#           that to scan the entire filesystem each time
#  - mandatory to manage properly the renaming of folders
#      (and the renaming of files contained in that folder, after renaming the folder)
def list2tree(filelist, setnormalchars):
	tree = {}
	fh = open(filelist,'rb')
	for line in fh.read().splitlines():
		try: 
			line_utf8 = line.decode("utf8")
		except:
			comps = line.split(b'/')
			cur = tree
			for comp in comps:
				enc_tips = None
				try:
					comp_utf8 = comp.decode("utf8")
					comp_enc = ""
				except:
					comp_utf8 = comp.decode("utf8", errors="backslashreplace")
					detenc = chardet.detect(comp)
					sel_encoding = None
					for encoding in {"latin1", "cp1252", "cp850", detenc['encoding']}:
						if encoding:
							line_enc = comp.decode(encoding, errors="ignore")
							line_enc_set = check_filename(line_enc, setnormalchars)
							if (sel_encoding is None) or (sel_encoding and (lenornone(line_enc_set) < lenornone(sel_enc_set))):
								sel_encoding = encoding
								sel_enc_set = line_enc_set
					comp_enc = comp.decode(sel_encoding, errors="ignore")
					if lenornone(sel_enc_set) > 0:
						enc_tips = "!!! No suitable encoding found !!! Encoding: " +  sel_encoding +" Set: " + str(sel_enc_set)
					else:
						enc_tips = "Encoding: " +  sel_encoding 
				if comp_utf8 in cur:
					cur = cur[comp_utf8]
				else:
					cur[comp_utf8]  = { '.' :  comp_enc }
					if enc_tips:
						cur[comp_utf8] ['..'] = enc_tips
					cur = cur[comp_utf8]
	clean_tree(tree)
	#pprint.pprint(tree)
	print(json.dumps(tree, indent=2,ensure_ascii=False))

# Recursive function to do the move of items in the tree
def do_mv(tree, cur):
	for key in list(tree.keys()):
		if (key != '.') and (key != '..'):
			item = tree[key]
			next = key
			if len(item['.']) > 0:
				key = backslashreplace_revert(key)
				src = cur.encode()+b'/'+ key
				dst = cur+'/'+item['.']
				print("mv ", src, " ", dst)
				os.rename(src,dst)
				next = item['.']
			do_mv(item, cur + "/" + next)

# Move the files by loading the tree file and starting the recursion
def json_do_mv(file):
	tree = json.load(open(file))
	do_mv(tree[''],"")

# Command line handling
parser = argparse.ArgumentParser(description='Helps the renaming of bad UTF8 filenames.')
parser.add_argument("command", help="the command to use", choices=['getchars', 'listsuspicious', 'listerrors', 'gettree', 'movetree'])
parser.add_argument("input", help="the input file")
args = parser.parse_args()

if args.command == "getchars":
	print(sorted(get_suspicious_chars(args.input, normalchars +  unwantedchars)))
elif args.command == "listsuspicious":
	list_suspicious_files(args.input, set(normalchars + unwantedchars))
elif args.command == "listerrors":
	list_utf8errors_files(args.input, set(normalchars + unwantedchars))
elif args.command == "gettree":
	list2tree(args.input, set(normalchars + unwantedchars))
elif args.command == "movetree":
	json_do_mv(args.input)


```

A noter que ce script est quand mÃªme assez expÃ©rimental / rudimentaire, et Ã  ne pas utiliser aveuglementâ€¦

Il existe un script [convmv](http://freshmeat.sourceforge.net/projects/convmv) disponible dans toutes les bonnes distributions qui permet de convertir assez simplement dâ€™un encoding en utf8. Pour les cas simples, câ€™est LE script Ã  regarder en premier. Jâ€™en avais fait il y a longtemps, Ã  lâ€™occasion de mon premier cleanup, une variante qui utilise le module [Encode::Guess](https://metacpan.org/pod/Encode::Guess) : [convmv-detect](/files/2020/05/convmv-detect.zip) (script perl Ã  tÃ©lÃ©charger et dÃ©zipper). Lâ€™usage est identique Ã  convmv, il suffit dâ€™indiquer â€œ-f detectâ€ pour demander la dÃ©tection et changer en ligne 449 la liste des encodings â€œsuspectsâ€ (oui câ€™est moche : câ€™est du perl et je suis nul en perlâ€¦)